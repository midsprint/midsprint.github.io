---
title: "Efficiently Cleaning Catapult Data"
---

```{r, warning=FALSE}
#| echo: false
suppressMessages(library(tidyverse))
suppressMessages(library(janitor))

url <- "https://github.com/aaronzpearson/midsprint-blog-data/blob/main/sample_catapult.csv?raw=true"

suppressMessages(sample_data <- read_csv(url))
```

## The problem

When working with raw catapult GPS or IMU data, you need to handle the first few lines of *meta-data* which includes info like date, time, and location of the session. This informaiton is less structured than you would normally find, being written in point-form. Below it are properly formatted variables and their subsequent observations.

## Load Libraries and Data

We'll be using the `tidyverse` and `janitor` packages for tidy code and efficient data cleaning.

```{r}
#| eval: false

library(tidyverse) # for tidy code
library(janitor) # efficient data cleaning
```

The data is available in my [GitHub repository](github.com/aaronzpearson/midsprint-blog-data) and can be loaded using the following code:

```{r}
#| eval: false

url <- "https://github.com/aaronzpearson/midsprint-blog-data/blob/main/sample_catapult.csv?raw=true"
sample_data <- read_csv(url)
```

### Data Structure

When looking at the first 10 rows of the data, you'll notice that the meta-data takes up the first 8 rows and that column names are not properly set.


```{r}
head(sample_data, 10)
```

## The Fix

We'll need to remove the first 8 rows. This can be achieved using functions like `slice()` or filtering row numbers. Otherwise, we can use take advantage of the `skip` argument from the `read_csv()` function. Partially cleaning the column names can also be achieved in the `read_csv()` function using the `col_types` argument.

```{r, warning=FALSE}
sample_data <- read_csv(url, skip = 8, col_types = cols())
```


### Updated Data Structure

You'll now notice that the meta-data is removed.

```{r}
head(sample_data)
```

Another issues arises that there are spaces in column names. This is handled effectively using `janitor`'s `clean_names()` function which sets names to lower case and special characters (periods, commas, spaces) as underscores.

We'll visualize the data within the same code block.

```{r}
sample_data <- read_csv(url, skip = 8, col_types = cols()) %>%
    janitor::clean_names()

head(sample_data, 10)
```

In the end, we've efficiently read-in the data, removed the meta-data, and cleaned the column names. This takes a handfull of lines of code and can be easily replicated.

## `read_catapult()`

The above steps can also be achieved by writing a custom function. This function will load the necessary libraries, read the data, skip the meta-data, and clean the column names with 1 or 2 lines of code.

### Function Breakdown

The function takes on two arguments: `file.path` and `skip.rows`. `file.path` is where the file is located, and `skip.rows` refers to the number of rows to skip while reading in the data. `skip.rows` is set to 8 by default but can be over-written as needed. Other arguments can be added to the function that mirror other arguments from the `read_csv()` and `clean_names()` functions. It is, for this example, assumed that the user wants to return the same outputs as those above.

I've opted to require the `readr` and `magrittr` packagse which are a part of the `tidyverse`. This is lighter-weight and doesn't require the entire `tidyverse` to be loaded when the function is initially run.

The rest of the steps are outlined within the function below using comments (starting with `#`).

```{r}
read_catapult <- function(file.path, skip.rows = 8) {
  
  # step 1: load libraries
  suppressMessages(require(readr))
  suppressMessages(require(janitor))
  suppressMessages(require(magrittr))
  
  # step 2: read-in the data
  temp <- readr::read_csv(file.path, skip = skip.rows, col_types = cols()) %>% 
    clean_names()
  
  # step 3: output cleaned data set
  return(temp)
  
}
```

### Comparing Methods

We should expect the `read_catapult()` function to return the same output as the method we used above. Although we can use the `identical()` function to compare the methods, it is very picky with *how* identical the outputs must be. What we'll do instead is a "hack" that will return whether values are identical between methods. Only the first 6 rows will be returned using the `head()` function. 

The file will be loaded from the GitHub repository (`url`).

```{r, message=FALSE, warning=FALSE}
url <- "https://github.com/aaronzpearson/midsprint-blog-data/blob/main/sample_catapult.csv?raw=true"

# initial method
readr_fn <- read_csv(url, skip = 8, col_types = cols()) %>%
    janitor::clean_names()

# read_catapul() function
custom_fn <- read_catapult(file.path = url, skip.rows = 8)
```

**The Moment of Truth**

```{r}
head(readr_fn == custom_fn)
```

