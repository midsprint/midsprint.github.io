[
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Midsprint",
    "section": "Education",
    "text": "Education\nMSc Exercise Science\nUniversity of Montreal | Montreal, QC\nBSc Kinesiology & Statistics\nSimon Fraser University | Vancouver, BC"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Midsprint",
    "section": "Experience",
    "text": "Experience\nSports Science Intern | Montreal Canadiens Hockey Club (NHL)\nSept 2020 - August 2022\nSports Science Consultant | Various collegiate and professional teams\nSept 2019 - Present"
  },
  {
    "objectID": "blog/advertisements/advert-button-format.html",
    "href": "blog/advertisements/advert-button-format.html",
    "title": "Midsprint",
    "section": "",
    "text": "..."
  },
  {
    "objectID": "blog/advertisements/affiliate-disclosure.html",
    "href": "blog/advertisements/affiliate-disclosure.html",
    "title": "Midsprint",
    "section": "",
    "text": "Affiliate Disclosure\n      \n    \n    \n      \n        Blog posts may contain affiliate links.\nPurchasing items through these links may result in me earning a small comission at no extra cost to you which helps support this website. See More"
  },
  {
    "objectID": "blog/advertisements/general-advert-format.html",
    "href": "blog/advertisements/general-advert-format.html",
    "title": "Midsprint",
    "section": "",
    "text": "...\n  \n\n\n  \n    \n      \n          Affiliate Disclosure\n      \n    \n    \n      \n        Blog posts may contain affiliate links.\nPurchasing items through these links may result in me earning a small comission at no extra cost to you which helps support this website. See More"
  },
  {
    "objectID": "blog/advertisements/general-offcanvas.html",
    "href": "blog/advertisements/general-offcanvas.html",
    "title": "Midsprint",
    "section": "",
    "text": "Dark Offcanvas Sidebar\n  Use the .text-bg-dark class to create a dark offcanvas menu.\n  Tip: We have also added the .btn-close-white class to .btn-close, to create a white close button that looks nice with the dark background:\n  \n    Open Offcanvas Sidebar"
  },
  {
    "objectID": "blog/advertisements/newsletter-signup-small.html",
    "href": "blog/advertisements/newsletter-signup-small.html",
    "title": "Midsprint",
    "section": "",
    "text": "We'll never share or sell your email address. By subscribing, you agree with Revue’s Terms of Service and Privacy Policy."
  },
  {
    "objectID": "blog/advertisements/newsletter-signup.html",
    "href": "blog/advertisements/newsletter-signup.html",
    "title": "Midsprint",
    "section": "",
    "text": "We'll never share or sell your email address. By subscribing, you agree with Revue’s Terms of Service and Privacy Policy."
  },
  {
    "objectID": "blog/advertisements/nsca-sports-sci.html",
    "href": "blog/advertisements/nsca-sports-sci.html",
    "title": "Midsprint",
    "section": "",
    "text": "The NSCA's Essentials of Sports Science is the industry's first comprehensive textbook on everything Sports Science.\n    \n    ...\n    \n    \n    \n    \n    \n    Get Your Copy of the NSCA's Essentials of Sports Science Here\n   \n    \n    Not sure that this is for you? You can also checkout these exerpts:\n    \n    ::: {.callout-note appearance=\"minimal\"}\n\n## Pay Attention\n\nUsing callouts is an effective way to highlight content that your reader give special consideration or attention.\n\n:::\n    \n    \n    \n  \n\n\n  \n    \n      \n          Affiliate Disclosure\n      \n    \n    \n      \n        Blog posts may contain affiliate links.\nPurchasing items through these links may result in me earning a small comission at no extra cost to you which helps support this website. See More"
  },
  {
    "objectID": "blog/affiliate-disclosure.html",
    "href": "blog/affiliate-disclosure.html",
    "title": "Midsprint",
    "section": "",
    "text": "Pick Up Your Copy of the NSCA's Essentials of Sport Science Here\n    \nLearn More\n\n\n  \n    \n  \n  \n    ..."
  },
  {
    "objectID": "blog/bdb_21.html#introduction",
    "href": "blog/bdb_21.html#introduction",
    "title": "Big Data Bowl 2021 Submission",
    "section": "1 Introduction",
    "text": "1 Introduction\nThe 40-yard dash is one of the most exciting events tested at the NFL Combine. Prospects set-up in a three-point stance and, after a brief pause, start of their own volition. Time begins when the hand is released from the ground and stops when the player breaks the plane at 40 yards. Sprint times are typically five seconds or less.\nIn those five seconds, players display their ability to produce force and power. Within position groups, the fastest players are drafted earlier and have more career success. Unfortunately, much of the data surrounding their career success and abilities are descriptive in nature. Research has also shown that the fastest 40-yard splits are run by players with the greatest top-speeds. Yet there is very little evidence that the 40-yard dash translates to an athlete’s on-field mechanical sprint abilities.\nPlayers rarely reach their top-speed during games. Passing plays that last 4 to 6 seconds challenge receivers to create space between themselves and the defender. It is their ability to juke, deke, cut, and feint that creates this space. Conversely, defenders attempt to remain in proximity of the receiver by mirroring these changes in speed and direction. These factors affect a player’s ability to reach their top-speed.\nThis paper explores the relationship between an athlete’s sprint kinetics and pass completion rates. Specifically, it aims to explore whether modeling an athlete’s mechanical sprint profile to cover a given distance can influence pass outcomes. Background information on modeling an athlete’s mechanical sprint abilities is provided as a framework for new methodologies that can be applied in-game.\n\n\n\n\n  \n\n\n\n    \n    \n  \n\n\n  \n    \nJoin the Midsprint Newsletter\n\n\n\n  \n  \n\n\n\nWe'll never share or sell your email address. By subscribing, you agree with Revue’s Terms of Service and Privacy Policy."
  },
  {
    "objectID": "blog/bdb_21.html#athlete-biomechanics",
    "href": "blog/bdb_21.html#athlete-biomechanics",
    "title": "Big Data Bowl 2021 Submission",
    "section": "2 Athlete Biomechanics",
    "text": "2 Athlete Biomechanics\nFurusawa and colleagues (1927) first modeled sprint kinetics and kinematics by analyzing a sprinter’s center of mass while running. Their approach utilizing inverse dynamics was a simple method of assessing a sprinter’s biomechanical abilities and horizontal force production. The methods Furusawa and colleagues (1927) introduced remain steadfast in the face of technological advancements.\nModeling sprint kinetics provides sports scientists with insight into a player’s lower-body neuromuscular potential. For example, this information provides practitioners with the understanding of an athlete’s strengths and weaknesses. Currently, player profiling is impractical when working with positional tracking data. Game data does not often comprise of players reaching maximal speeds with maximal effort, a key factor in force-velocity-power profiling. Instead, researchers resort to profiling athletes at the Combine’s 40-yard dash event. They hope that these athlete profiles translate to in-game sprint performance.\nAn emerging field in sports science is in-situ player profiling. This concept employs positional tracking data to model a player’s force-velocity-power profile. Doing so provides insight into how events like the 40-yard dash translate to on-field mechanical sprint capabilities. Presently there is only one publication, a proof-of-concept in men’s professional soccer, that utilizes GPS data to model force-velocity profiles. This submission expands on the in-situ concept in professional American Football.\n\n2.1 Sprint Kinetics\nThere is very little difference between prospects’ mechanical abilities; within all position groups, fast and slow players express the same sprint kinetics. The area in which players differ is their ability to accelerate. Furusawa et al. (1927) discovered that all humans reach their maximal speeds following the function:\n\\[\nv(t) = v_\\max \\cdot (1-e^\\frac{-t}{\\tau})\\\\\n\\]\nwhere \\(v(t)\\) represents an athlete’s velocity at time \\(t\\), \\(v_\\max\\) is an athlete’s maximum speed, and \\(\\tau\\) is the acceleration constant calculated:\n\\[\n\\tau = \\frac{v_\\max} {a_\\max}\n\\]\nwhere \\(a_\\max\\) is an athlete’s maximum acceleration. Building on the functions above, acceleration was modeled:\n\\[\na(t) = (\\frac {v_\\max}{\\tau}) \\cdot e^\\frac{-t}{\\tau}\\\\\n\\]\nwhere \\(a(t)\\) represents acceleration at time \\(t\\). Distance covered over time was also modeled:\n\\[\n\\Delta d (t)= v_\\max \\cdot ((t + \\tau) \\cdot e^\\frac{-t}{\\tau}) - (v_\\max \\cdot \\tau)\n\\]\nwhere \\(\\Delta d (t)\\) represents the distance travelled from point \\(d_0\\) to \\(d_i\\) at time \\(t\\).\nThe plot below shows two athletes: athlete A (red) is acceleration dominant whereas athlete B (black) is speed dominant. Although these athletes are different, it is the context of a play that displays their strengths.\n\n\n\n\n\n\n\n\n\n2.2 Modernizing Old Models\nTo apply the models above, athletes must begin with zero velocity. This translates well to football because, all but one offensive athlete, cannot move until the ball is snapped. Unfortunately, unlike track sprinting, receivers rarely run linearly at maximal efforts. This hurdle makes it difficult to apply player profiling models in-game. If receivers are impeded during play, they risk losing speed and require more time to reach maximal velocity.\nThe concept is simple: which athlete reaches the intended destination first? Answering this question hinges on comparing athletes’ maximal potentials to cover distances in the time between the quarterback’s release of the ball and its arrival location. Thus, the models must be adjusted for time to be a function of velocity, acceleration, and distance.\nTime as a function of velocity, detailing how long it takes an athlete to reach a given speed:\n\\[\nv^{-1}(t) = ln[1- (\\frac{v}{v_\\max})]\\cdot\\tau\n\\]\nwhere \\(v\\) is an athlete’s current speed and \\(0 \\leq v < v_\\max\\).\nTime as a function of distance, illustrating how long it takes to reach a position in space:\n\\[\n\\Delta d^{-1}(t) = \\tau \\cdot W(-e^\\frac{1 - \\Delta d}{\\tau \\cdot v_\\max}) + \\tau + \\frac{\\Delta d}{v_\\max}\n\\]\nwhere \\(W\\) is the Lambert W function and defined as the inverse of \\(xe^x\\). That is:\n\\[\ny=W(x)\\iff x=ye^y\n\\]\nFinally, to model the speed reached, having travelled a given distance, the function is as follows:\n\\[\nv^{-1}(\\Delta d) = v_\\max \\cdot (t + \\tau  e^\\frac{-t}{\\tau}) - (v_\\max \\cdot \\tau)\n\\]\nwhere \\(t\\) is equal to \\(v^{-1}(t)\\).\n\n\n2.3 Midsprint\nWhen the quarterback releases the ball, athletes rarely have zero velocity. The models above depend on individuals starting their run from a dead start. To understand how long it will take an athlete to cover a distance when in motion, the following function was derived:\n\\[\nv^{-1}(\\Delta d^{-1} (t)) = [\\tau \\cdot W(-e^\\frac{1 - \\Delta d + d_0}{\\tau \\cdot v_\\max}) + \\tau + \\frac{\\Delta d + d_0}{v_\\max}] - [\\tau \\cdot W(-e^\\frac{1 - d_0}{\\tau \\cdot v_\\max}) + \\tau + \\frac{d_0}{v_\\max}]\n\\]\nwhere \\(d_0\\) is equal to \\(v^{-1}(\\Delta d)\\) when \\(v = 0\\). This function assumes that athletes are running at maximal effort when spanning distances."
  },
  {
    "objectID": "blog/bdb_21.html#predicting-pass-outcomes",
    "href": "blog/bdb_21.html#predicting-pass-outcomes",
    "title": "Big Data Bowl 2021 Submission",
    "section": "3 Predicting Pass Outcomes",
    "text": "3 Predicting Pass Outcomes\n\n3.1 Nearest Defender\nThe defender closest to the receiver when the ball arrived was considered the primary defender. At the time of the ball’s release, players had unique locations on the field. Distances were calculated for the receiver and primary defender from their respective positions to where the ball arrived. The time interval between the release and arrival of the football was also recorded.\nThe assumption dictates that the nearest defender was also the primary defender. Although this assumption may not always apply, it simplifies analyses.\n\n\n3.2 Calculating Outcomes\nBased on player speed and field position, the least amount of time required to travel from their current position to the ball’s destination was modeled. The time required to run this distance was based on the athlete’s unique force-velocity-power profile. As such, modeled outcomes are founded on the athlete arriving at the intended position with maximal effort and without impediment. Finally, pass outcomes were decided on whichever athlete arrived at the target destination first (offense: completion, defense: incompletion).\nAll analyses were completed in R 4.0.2."
  },
  {
    "objectID": "blog/bdb_21.html#results",
    "href": "blog/bdb_21.html#results",
    "title": "Big Data Bowl 2021 Submission",
    "section": "4 Results",
    "text": "4 Results\nThere were 14,271 pass attempts that fit the inclusion criteria (11,022 completions, 3,249 incompletions). The average distance from the time-of-release to time-of-arrival was 5.32 yards and 7.89 yards for receivers and defenders, respectively. Receivers had a mean top speed of 10.6 yd/s and acceleration rate of 12.95 y/s/s (\\(\\tau\\) = 0.82 s), whereas defenders displayed a mean top speed of 10.26 yd/s and acceleration of 9.55 y/s/s (\\(\\tau\\) = 1.07 s). All details can be found in the table below.\nThe model correctly predicted pass outcomes 77.48% of the time (96.38% when the pass was successful, 14.41% when incomplete)."
  },
  {
    "objectID": "blog/bdb_21.html#discussion",
    "href": "blog/bdb_21.html#discussion",
    "title": "Big Data Bowl 2021 Submission",
    "section": "5 Discussion",
    "text": "5 Discussion\nThis paper was influenced by the current literature surrounding force-velocity-power profiling from the NFL Combine’s 40-yard dash test. It provides a novel approach to modeling athletes’ mechanical sprint capabilities that utilizes positional tracking data. Presently, there is only one publication that also attempts to model athletes in-situ.\nOutcomes were recorded as whichever athlete arrived first at the intended destination, based on their unique profile. The model accurately predicted pass outcomes nearly 4 out of 5 plays. Modeling pass outcomes in this paper presented unique challenges. The model cannot take game factors into account like interference, routes, contact, jukes, and dekes. Rather, it assumes that players are running linearly at maximal effort, without obstructions. As demonstrated, the outcomes heavily favoured pass completions. This can be attributed to the fact that the receiver knows their routes while defenders lag and must react to a receiver’s movements.\n\n5.1 Positional F-V-P Profiles\nSecondary analyses revealed that receivers are acceleration-dominant. Wide receivers and tight ends are heavily acceleration-dominant with \\(\\tau\\) below 0.90 s. All other offensive positions are also acceleration-dominant but to a lesser degree. Defensively, only cornerbacks and safeties are acceleration-dominant with very similar maximal speed, maximal acceleration, and \\(\\tau\\) values (0.83 s) as receivers. All other defensive positions display balanced or speed-dominant profiles.\n\n\n5.2 Tuning the Model\nA large proportion of receivers decelerate when receiving a pass. This deceleration might be due to quarterbacks poorly leading receivers while in stride, or the receiver’s attempt to secure the football. It was also found that most defenders accelerate increase between ball release and arrival. Defenders were also found to increase their speeds between ball arrival and pass reception. These factors affected pass completion rates.\nThe model inaccurately predicts incompletions nearly 85% of the time. Further exploration identified that when the defender lags a receiver by less than 0.51 seconds to the point of arrival, as modeled when starting with zero velocity, it significantly decreases pass completion rates. Ergo, the proximity of a defender to the receiver coupled with the receiver’s deceleration dictates that the defense heavily influences pass outcomes between ball arrival and ball securement.\nThe model was rerun with the parameter that passes marked incomplete if the defender lags the receiver by a maximum of 0.51 seconds to the point of arrival. Results from this model increased the accuracy of identifying pass incompletions nearly 55%, and improved overall accuracy to 87.7%."
  },
  {
    "objectID": "blog/bdb_21.html#practical-applications",
    "href": "blog/bdb_21.html#practical-applications",
    "title": "Big Data Bowl 2021 Submission",
    "section": "6 Practical Applications",
    "text": "6 Practical Applications\nThe 40-yard dash lasts 5 seconds or less. Much of the current research states that players with higher top speeds have better career success. The research does not explore the correlation between results at the Combine and on-field performance. This paper illustrates that wide receivers,tight ends, and corner acks heavily rely on their rates of acceleration in-game. Moreover, they rarely reach speeds on the field that were reached during the NFL Combine’s 40-yard dash test. Scouts should focus on an athlete’s rate of acceleration, not their top-speed or 40-yard split, when assessing athletic abilities. The formulas presented in this paper, and the accompanying R package midsprint, provides scouts with a novel method of assessing prospects.\n\n\n\n\n  \n  \n    \n  \n  \n  \n    Join the Midsprint Newsletter\n  \n  \n\nWe'll never share or sell your email address. By subscribing, you agree with Revue’s Terms of Service and Privacy Policy."
  },
  {
    "objectID": "blog/bdb_22.html",
    "href": "blog/bdb_22.html",
    "title": "Big Data Bowl 2022 Submission",
    "section": "",
    "text": "The angle-velocity trade-off is the interplay between a player’s current speed and their ability to change direction. As such, we can theoretically identify the greatest speed an athlete can maintain for any given directional change. For example, if the punt returner catches the ball while in motion, they can maximize their change-of-direction ability which can help them effectively juke oncoming gunners. In doing so, the punt returner maximizes their punt return yardage. Punt returners who catch the ball while in motion also have increased risks of mistiming the catch and fumbling the ball. Therefore, this submission suggests that the punt returner should lateral the ball to a secondary player who is in motion to maximize return yardage while mitigating the risks.\n\n\n\n\n  \n\n\n\n    \n    \n  \n\n\n  \n    \nJoin the Midsprint Newsletter\n\n\n\n  \n  \n\n\n\nWe'll never share or sell your email address. By subscribing, you agree with Revue’s Terms of Service and Privacy Policy."
  },
  {
    "objectID": "blog/bdb_22.html#background",
    "href": "blog/bdb_22.html#background",
    "title": "Big Data Bowl 2022 Submission",
    "section": "Background",
    "text": "Background\nWhile the ball is in the air, the punt returner is looking up and receives information from teammates about oncoming gunners, play formations, and potential running routes. Once caught, the punt returner looks downfield, processes the information, and decides on their best course-of-action. If the punt returner has zero velocity at the time of the catch, they have limited ability to juke oncoming gunners. Instead, if the punt returner is provided cues from a teammate positioned obliquely on whether a lateral is appropriate, and the direction and distance of the lateral, the secondary player can process information about oncoming defenders effectively. In doing so, the secondary player can better process running routes while also maximizing their ability to effectively juke oncoming gunners.\nThis submission expands on previous work that modelled a player’s mechanical sprint ability (https://www.kaggle.com/aaronzpearson/modeling-player-biomechanics). The model introduced in this submission is based on the following paper which quantifies a player’s change-of-direction ability: https://link.springer.com/article/10.1007%2Fs40279-018-0968-3. The authors state that a player’s intended change-of-direction impacts the maximal speed that they can maintain. For example, if a player is moving too quickly, they must decelerate to effectively manoeuvre on the field. The image below outlines the magnitude to which players must decelerate from maximal speeds to execute a given change-in-direction.\n Credit: Dos’Santos et al., 2018\nTherefore, this submission hypothesizes that a secondary player can earn greater return yardage than the punt returner. The hypothesis is based on the model proposed by Don’Santos et al., 2018 wherein a player in motion has a greater ability to change-direction and evade oncoming gunners."
  },
  {
    "objectID": "blog/bdb_22.html#methods",
    "href": "blog/bdb_22.html#methods",
    "title": "Big Data Bowl 2022 Submission",
    "section": "Methods",
    "text": "Methods\nData from the 2018-2019, 2019-2020, and 2020-2021 seasons were used to model a player’s change-of-direction ability. While cleaning the data, a player’s orientation was adjusted so that an orientation of 0 degrees resulted in the player facing the left goal-line. Orientation was then adjusted so that a player can rotate 0 to 180 degrees in either direction. Speed was binned every 0.2 yards/ second (i.e. (0, 0.2], (0.2, 0.4], etc.) and the greatest rate-of-change in orientation was extracted from the data set. Finally, a 3rd degree polynomial was fit to the data.\n\nModelling Change-of-Direction Ability\nThe change-of-direction ability between the gunner and secondary player were compared to assess the proposed model.\nThe secondary player was placed obliquely to the punt returner with 2.5 yards of lateral separation. They were assumed to run perpendicular to the punt returner and receive the ball 0.3 seconds after the punt is caught, parallel to the punt returner. The secondary player was also assumed to have a speed of 2.5 yards/ second (slow jog) when they received the lateral. Once the ball was secured, the secondary player was unimpeded in their maximal sprint efforts in any direction. The secondary player’s sprint abilities were previously modelled here [1]:\n\\[\nv(t) = v_\\max \\cdot (1-e^\\frac{-t}{\\tau})\\\\\n\\]\nwhere \\(v(t)\\) represents an athlete’s velocity at time \\(t\\), \\(v_\\max\\) is an athlete’s maximum speed, and \\(\\tau\\) is the acceleration constant calculated:\n\\[\n\\tau = \\frac{v_\\max} {a_\\max}\n\\]\nwhere \\(a_\\max\\) is an athlete’s maximum acceleration.\nThe secondary player’s sprint abilities were set as having a max speed of 10 yards/ second, a max acceleration of 15 yards/ second/ second, and a resulting acceleration constant of 0.67 seconds.\nFinally, the ability for a gunner to change direction was assessed. The gunner was assumed to be maintaining a consistent speed until the secondary player received the lateral. Once the secondary player has secured the ball and changed direction, the gunner would decelerate to the maximal speed for the intended change-of-direction to intercept the secondary player. The rate of deceleration over a given distance was modelled based on that of acceleration [2].\n\\[\n\\Delta d (t)= v_\\max \\cdot ((t + \\tau) \\cdot e^\\frac{-t}{\\tau}) - (v_\\max \\cdot \\tau)\n\\]\nwhere \\(\\Delta d(t)\\) represents the distance travelled from point \\(d_0\\) to \\(d_i\\) at time \\(t\\).\nTo model deceleration, the inverse of formula 1 was adjusted so that it took on a player’s current speed and goal speed (goal speed < current speed) [3]:\n\\[\nv^{-1}(t) = ln[1- (\\frac{v_i}{0.1} - 1)]\\cdot\\tau - ln[1 - (\\frac{v_n}{0.1} - 1)]\n\\]\nwhere \\(v_i\\) is the player’s current speed and \\(v_n\\) is the player’s goal speed. [2] was also adjusted so that the player’s max speed was replaced with the player’s current speed, and the time split, \\(t\\) was that from [3].\n\\[\n\\Delta d (t)= v_i \\cdot (t + \\tau \\cdot e^\\frac{-t}{\\tau}) - (v_i \\cdot \\tau)\n\\]"
  },
  {
    "objectID": "blog/bdb_22.html#results",
    "href": "blog/bdb_22.html#results",
    "title": "Big Data Bowl 2022 Submission",
    "section": "Results",
    "text": "Results\n\nReturn Yardage\nReturn yardage was plotted against the punt returner’s initial velocity. The plot shows that the greatest return distances during the 2018-2019 (black), 2019-2020 (red), and 2020-2021 (blue) seasons occurred when the punt returner was moving between 1 – 4 yards/ second. Please note that fair catches were removed from this plot.\n\n\n\nChange of Direction\nChange of direction was capped at 90 degrees because it is unrealistic that a change-in-direction greater than such is feasible at 10Hz. Rather, it was assessed that such changes in direction were due to player impacts and improper data collection. The greatest rate-of-change was then modelled. The resulting model from the 2018-2019 (black) season followed the polynomial:\n\\[\ncod(v) = -15.85v + 4.41v^2 - 0.23v^3 + 63.89\n\\]\nwhere \\(cod(v)\\) is the player’s change-of-direction ability at a given speed, and \\(v\\) is a player’s speed in yards/ second. Models were similarly fit for the 2019-2020 and 2020-2021 seasons.\n\nThe model assumed that the player’s change-of-direction ability did not differ between left and right. Therefore, the player’s abilities resulted as the following (note that the axes are flipped for aesthetic purposes):\n\n\n\nRate of Deceleration\nThe player’s rate of deceleration was assessed by extracting the greatest speed an athlete maintained during their deceleration. The plot below shows a similar trend to those above: a player’s deceleration ability does not change by a large magnitude between seasons.\n\n\n\nSprint Abilities\nFinally, the gunner’s sprint abilities were modelled and fit, as demonstrated in the plot below:"
  },
  {
    "objectID": "blog/bdb_22.html#sample-plays",
    "href": "blog/bdb_22.html#sample-plays",
    "title": "Big Data Bowl 2022 Submission",
    "section": "Sample Plays",
    "text": "Sample Plays\nUsing the 2018-2019 data, a gunner’s median distance from the punt returner at the time of reception was 12.4 yards. Furthermore, at the time of reception, gunners had a median speed of 8.4 yards/ seconds, acceleration of 5.6 yards/ second/ second, and an acceleration constant that was modelled of 1.4 seconds. Therefore, the gunner’s maximal sprint abilities would allow them to reach the punt returner in 1.51 seconds. These values were utilized in the example below.\n\nIn this example, the gunner has an initial maximum ability to change direction of 24.4 degrees. Within the secondary player’s allotted 1.21 seconds (1.51 seconds for the gunner to reach the punt returner minus 0.3 seconds for the lateral), they travelled linearly 7.83 yards. To intercept the secondary player, the gunner must change direction relative to the secondary player at 49.5 degrees. Therefore, the gunner must decelerate to 4.88 yards/ second which will take 0.77 seconds and 1.60 yards. This provides the gunner with 0.73 seconds to re-accelerate to reach the secondary player. In the example above, the time it takes to change direction and span the required distance takes a total of 2.30 seconds. The secondary player has ample time to continue their given path and evade the gunner.\n\nFurther Results"
  },
  {
    "objectID": "blog/bdb_22.html#conclusions-and-future-work",
    "href": "blog/bdb_22.html#conclusions-and-future-work",
    "title": "Big Data Bowl 2022 Submission",
    "section": "Conclusions and Future Work",
    "text": "Conclusions and Future Work\nThe model built for this submission clearly indicates that the punt returner should lateral the ball to a secondary player to improve return yardage. Further research can model the impact of having the punt returner act as a blocker after they lateral the ball which can provide the secondary player greater ability to return the ball. Finally, there is reason to believe that a secondary player provides an option for the receiving team in that the gunner must choose a player to attack. As such, the punt returner can decide which player the gunner has chosen and lateral the ball when appropriate.\n\nLimitations\nThis submission is based in theory. Unfortunately, there are no observed plays where the punt returner immediately laterals the ball. Therefore, it is impractical to validate these findings.\n\n\n\n\n  \n  \n    \n  \n  \n  \n    Join the Midsprint Newsletter\n  \n  \n\nWe'll never share or sell your email address. By subscribing, you agree with Revue’s Terms of Service and Privacy Policy."
  },
  {
    "objectID": "blog/blog-posts.html",
    "href": "blog/blog-posts.html",
    "title": "Blog Posts",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "blog/cardiac-drift.html",
    "href": "blog/cardiac-drift.html",
    "title": "Modelling Cardia Drift",
    "section": "",
    "text": "Using Linear Regressions to Model Cardia Drift Using GPS Data"
  },
  {
    "objectID": "blog/cardiac-drift.html#tracking-heart-rate",
    "href": "blog/cardiac-drift.html#tracking-heart-rate",
    "title": "Modelling Cardia Drift",
    "section": "Tracking Heart Rate",
    "text": "Tracking Heart Rate\nSmart watches provide a somple method to track your heart rate throughout your day. Heart rate can provide feedback on sleep quality, training readiness, and overall recovery rates.\nEndurance athletes also use heart rate to estimate their max aerobic capacity (VO2max) and training intensity levels. During extended training sessions, there is an upward trend in athletes’ heart rate while working at the same intensities. This phenomenon is called cardiac drift.\n\n\n\n\n  \n\n\n\n    \n    \n  \n\n\n  \n    \nJoin the Midsprint Newsletter\n\n\n\n  \n  \n\n\n\nWe'll never share or sell your email address. By subscribing, you agree with Revue’s Terms of Service and Privacy Policy.\n\n\nCardiac Drift\nThe upward trend in an athlete’s heart rate is multifactorial. The primary causes are dehydration and accumulation of metabolites.\nDehydration decreases blood plasma levels which forces the heart to work harder to deliver the necessary blood supply to the working muscles. The decreased blood plasma levels also affects the ability for the blood to accept metabolites that accumulate in working muscles.\nTo contract, muscles require energy. Endurance athletes predominantly rely on aerobic energy sources like carbohydrates and fat. The breakdown of these energy sources releases by-products that interfere with how efficient the muscle can contract. Therefore, it is important for these by-products to be expelled from the muscle cells. As water levels in the blood diminish, there is less drive for these by-products to exit the cell. Instead, they interfere with the mechanisms in the muscle cell that allow it to contract. This results in less forceful contractions and slower speeds.\nWhen an athlete wants to maintain a given pace, they need to overcome these by-products. To do so, they heart increases its pace to deliver more energy to the muscles and maintain the needed amount of force.\n\n\nLoad the packages\n\nlibrary(tidyverse) # for tidy code"
  },
  {
    "objectID": "blog/cardiac-drift.html#load-the-data",
    "href": "blog/cardiac-drift.html#load-the-data",
    "title": "Modelling Cardia Drift",
    "section": "Load the Data",
    "text": "Load the Data\nThe data is available on my GitHub repository and can be downloaded using the code below.\n\nurl <- \"https://github.com/aaronzpearson/midsprint-blog-data/raw/main/2017_01_31_18_36_44.csv\"\n\ngps_data <- read_csv(url)\n\nThe data structure:\n\nsummary(gps_data)\n\n      secs              km        power             hr             cad        \n Min.   :   0.0   Min.   :0   Min.   :  0.0   Min.   :113.0   Min.   :  0.00  \n 1st Qu.: 986.8   1st Qu.:0   1st Qu.:193.0   1st Qu.:177.0   1st Qu.: 92.00  \n Median :1973.5   Median :0   Median :206.0   Median :182.0   Median : 93.00  \n Mean   :1973.5   Mean   :0   Mean   :196.8   Mean   :178.7   Mean   : 90.95  \n 3rd Qu.:2960.2   3rd Qu.:0   3rd Qu.:216.0   3rd Qu.:185.0   3rd Qu.: 94.00  \n Max.   :3947.0   Max.   :0   Max.   :783.0   Max.   :190.0   Max.   :158.00  \n      alt          \n Min.   :-3.00000  \n 1st Qu.:-1.00000  \n Median : 0.00000  \n Mean   :-0.02067  \n 3rd Qu.: 1.00000  \n Max.   : 2.40000"
  },
  {
    "objectID": "blog/cardiac-drift.html#cleaning-the-data",
    "href": "blog/cardiac-drift.html#cleaning-the-data",
    "title": "Modelling Cardia Drift",
    "section": "Cleaning the Data",
    "text": "Cleaning the Data\nTo clean the data, we’ll remove when the athlete did not record any data.\n\ngps_data <- gps_data %>% # write over initial data set\n                         # not often recommended, but safe in this case\n    filter(\n        power > 0, # only keep data when the athlete was working\n        hr > 0 # remove data where the hr monitor was not being worn\n        )"
  },
  {
    "objectID": "blog/cardiac-drift.html#visualizing-the-data",
    "href": "blog/cardiac-drift.html#visualizing-the-data",
    "title": "Modelling Cardia Drift",
    "section": "Visualizing the Data",
    "text": "Visualizing the Data\nPlotting heart rate and power versus time.\n\noptions(repr.plot.width = 14, repr.plot.height = 8) # set plot size\n\ntheme_set(theme_minimal()) # set the global theme\n\nggplot(gps_data, aes(x = secs)) + # only set x-axis because of 2 dependent variables\n    geom_point(aes(y = power), color = \"black\") + \n    geom_point(aes(y = hr), colour = \"red\") + \n    xlab(\"Time (s)\") + ylab(\"Power (W), Heart Rate (bpm)\")"
  },
  {
    "objectID": "blog/cardiac-drift.html#the-linear-model",
    "href": "blog/cardiac-drift.html#the-linear-model",
    "title": "Modelling Cardia Drift",
    "section": "The Linear Model",
    "text": "The Linear Model\nBelow is some code to visualize an athlete’s increase in heart-rate while working at nearly the same rate for 45 minutes.\n\nfit <- lm(hr ~ power:secs, data = gps_data) # only includes power-time interaction term\n\nsummary(fit) # overview of model fit\n\n\nCall:\nlm(formula = hr ~ power:secs, data = gps_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-83.284  -1.073   2.029   4.234  19.125 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 1.694e+02  2.605e-01   650.3   <2e-16 ***\npower:secs  2.522e-05  5.771e-07    43.7   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.43 on 3820 degrees of freedom\nMultiple R-squared:  0.3333,    Adjusted R-squared:  0.3332 \nF-statistic:  1910 on 1 and 3820 DF,  p-value: < 2.2e-16\n\n\nAlthough the goodness-of-fit isn’t great (r^2: 0.33, RSE: 8.43), it is inconsequential for the rest of the example.\nWe can add the fitted model to the current data set and then visualize the outputs.\n\ngps_data$fitted <- fitted(fit)\n\nBelow, you’ll note that the fitted model is not linear. This is because of the interaction between power and time. That is, when there is a spike in the interaction term, we’d anticipate a spike in heart rate as well.\nIf we did not include the interaction term and only included time, the model would be linear. This is seen in the green line that is fit using geom_smooth and the secs variable.\n\nggplot(gps_data, aes(x = secs)) + # only set x-axis because of 2 dependent variables\n    geom_point(aes(y = power), color = \"black\") + \n    geom_point(aes(y = hr), colour = \"red\") + \n    geom_point(aes(y = fitted), colour = \"blue\") + # fitted values\n    geom_smooth(aes(y = hr), colour = \"green\", size = 0.5, method = \"lm\") + # uses \"lm\" (linear model) to fit trendline\n    xlab(\"Time (s)\") + ylab(\"Power (W), Heart Rate (bpm)\")"
  },
  {
    "objectID": "blog/cardiac-drift.html#predicted-outcomes",
    "href": "blog/cardiac-drift.html#predicted-outcomes",
    "title": "Modelling Cardia Drift",
    "section": "Predicted Outcomes",
    "text": "Predicted Outcomes\nUsing some quick math, we can estimate the athlete’s heart rate, Since there are spikes in power output, we’ll e conservative and use its 75th percentile.\n\nquantile(gps_data$power); quantile(gps_data$secs)\n\n  0%  25%  50%  75% 100% \n   7  195  206  217  783 \n\n\n     0%     25%     50%     75%    100% \n   0.00  979.25 1934.50 2914.75 3922.00 \n\n\nManually calculating the athlete’s fitted heart rate 3/4 through the race results in an increase of 16 which results in an increase from 169 to approximately 184 beats per minute while the athlete works at 217 kW.\nIn the end, we see that the athlete’s heart rate is expected to increase by 16 beats per minute after 45-50 minutes of sustained exercise."
  },
  {
    "objectID": "blog/cardiac-drift.html#application",
    "href": "blog/cardiac-drift.html#application",
    "title": "Modelling Cardia Drift",
    "section": "Application",
    "text": "Application\nThis information can be applied to the critical velocity model. The external expression of an increase in heart rate is the decrease in an athlete’s critical velocity. As such, we should expect to see that the athlete either relies more on anaerobic energy over time to sustain a given pace or that their pace decreases to maximize their use of aerobic energy.\n\n\n\n\n  \n  \n    \n  \n  \n  \n    Join the Midsprint Newsletter\n  \n  \n\nWe'll never share or sell your email address. By subscribing, you agree with Revue’s Terms of Service and Privacy Policy."
  },
  {
    "objectID": "blog/clean-catapult-data.html#the-problem",
    "href": "blog/clean-catapult-data.html#the-problem",
    "title": "Efficiently Cleaning Catapult Data",
    "section": "The problem",
    "text": "The problem\nWhen working with raw catapult GPS or IMU data, you need to handle the first few lines of meta-data which includes info like date, time, and location of the session. This informaiton is less structured than you would normally find, being written in point-form. Below it are properly formatted variables and their subsequent observations.\n\n\n\n\n  \n\n\n\n    \n    \n  \n\n\n  \n    \nJoin the Midsprint Newsletter\n\n\n\n  \n  \n\n\n\nWe'll never share or sell your email address. By subscribing, you agree with Revue’s Terms of Service and Privacy Policy."
  },
  {
    "objectID": "blog/clean-catapult-data.html#load-libraries-and-data",
    "href": "blog/clean-catapult-data.html#load-libraries-and-data",
    "title": "Efficiently Cleaning Catapult Data",
    "section": "Load Libraries and Data",
    "text": "Load Libraries and Data\nWe’ll be using the tidyverse and janitor packages for tidy code and efficient data cleaning.\n\nlibrary(tidyverse) # for tidy code\nlibrary(janitor) # efficient data cleaning\n\nThe data is available in my GitHub repository and can be loaded using the following code:\n\nurl <- \"https://github.com/aaronzpearson/midsprint-blog-data/blob/main/sample_catapult.csv?raw=true\"\nsample_data <- read_csv(url)\n\n\nData Structure\nWhen looking at the first 10 rows of the data, you’ll notice that the meta-data takes up the first 8 rows and that column names are not properly set.\n\nhead(sample_data, 10)\n\n# A tibble: 10 × 9\n   # OpenField Export : 1/01/…¹  ...2  ...3   ...4  ...5  ...6  ...7  ...8  ...9\n   <chr>                        <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 \"# Reference time : 12/12/1… NA     NA   NA     NA     NA     NA     NA    NA\n 2 \"# CentisecTime : 123456789… NA     NA   NA     NA     NA     NA     NA    NA\n 3 \"# DeviceId : 00000\"         NA     NA   NA     NA     NA     NA     NA    NA\n 4 \"# Speed Units : Miles Per … NA     NA   NA     NA     NA     NA     NA    NA\n 5 \"# Distance Units : Yards\"   NA     NA   NA     NA     NA     NA     NA    NA\n 6 \"# Period: \\\"Sample Game\\\"\"  NA     NA   NA     NA     NA     NA     NA    NA\n 7 \"# Athlete: \\\"John Doe\\\"\"    NA     NA   NA     NA     NA     NA     NA    NA\n 8 \"Timestamp\"                  NA     NA   NA     NA     NA     NA     NA    NA\n 9 \"12:00:01\"                    0     12.7  0.379  0     11.1  100.     0     0\n10 \"12:00:02\"                    0.09  12.8  0.224  0.61  11.1  100.     0     0\n# … with abbreviated variable name\n#   ¹​`# OpenField Export : 1/01/1901 12:00:00 PM`"
  },
  {
    "objectID": "blog/clean-catapult-data.html#the-fix",
    "href": "blog/clean-catapult-data.html#the-fix",
    "title": "Efficiently Cleaning Catapult Data",
    "section": "The Fix",
    "text": "The Fix\nWe’ll need to remove the first 8 rows. This can be achieved using functions like slice() or filtering row numbers. Otherwise, we can use take advantage of the skip argument from the read_csv() function. Partially cleaning the column names can also be achieved in the read_csv() function using the col_types argument.\n\nsample_data <- read_csv(url, skip = 8, col_types = cols())\n\n\nUpdated Data Structure\nYou’ll now notice that the meta-data is removed.\n\nhead(sample_data)\n\n# A tibble: 6 × 9\n  Timestamp Seconds Velocity Accelerat…¹ Odome…² Latit…³ Longi…⁴ Heart…⁵ Playe…⁶\n  <time>      <dbl>    <dbl>       <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1 12:00:01     0        12.7     0.379      0       11.1   100.        0     0  \n2 12:00:02     0.09     12.8     0.224      0.61    11.1   100.        0     0  \n3 12:00:03     0.19     12.8     0.127      1.22    11.1   100.        0     0.1\n4 12:00:04     0.29     12.8     0.0628     1.79    11.1   100.        0     0.1\n5 12:00:05     0.39     12.7     0.00756    2.35    11.2   100.        0     0.1\n6 12:00:06     0.49     12.6    -0.0612     2.97    11.2    99.9       0     0.2\n# … with abbreviated variable names ¹​Acceleration, ²​Odometer, ³​Latitude,\n#   ⁴​Longitude, ⁵​`Heart Rate`, ⁶​`Player Load`\n\n\nAnother issues arises that there are spaces in column names. This is handled effectively using janitor’s clean_names() function which sets names to lower case and special characters (periods, commas, spaces) as underscores.\nWe’ll visualize the data within the same code block.\n\nsample_data <- read_csv(url, skip = 8, col_types = cols()) %>%\n    janitor::clean_names()\n\nhead(sample_data, 10)\n\n# A tibble: 10 × 9\n   timestamp seconds velocity accelera…¹ odome…² latit…³ longi…⁴ heart…⁵ playe…⁶\n   <time>      <dbl>    <dbl>      <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 12:00:01     0        12.7    0.379      0       11.1   100.        0     0  \n 2 12:00:02     0.09     12.8    0.224      0.61    11.1   100.        0     0  \n 3 12:00:03     0.19     12.8    0.127      1.22    11.1   100.        0     0.1\n 4 12:00:04     0.29     12.8    0.0628     1.79    11.1   100.        0     0.1\n 5 12:00:05     0.39     12.7    0.00756    2.35    11.2   100.        0     0.1\n 6 12:00:06     0.49     12.6   -0.0612     2.97    11.2    99.9       0     0.2\n 7 12:00:07     0.59     12.4   -0.152      3.51    11.2    99.9       0     0.3\n 8 12:00:08     0.69     12.2   -0.258      4.07    11.2    99.9       0     0.3\n 9 12:00:09     0.79     12.0   -0.373      4.62    11.2    99.9       0     0.4\n10 12:00:10     0.89     11.7   -0.498      5.06    11.2    99.9       0     0.5\n# … with abbreviated variable names ¹​acceleration, ²​odometer, ³​latitude,\n#   ⁴​longitude, ⁵​heart_rate, ⁶​player_load\n\n\nIn the end, we’ve efficiently read-in the data, removed the meta-data, and cleaned the column names. This takes a handfull of lines of code and can be easily replicated."
  },
  {
    "objectID": "blog/clean-catapult-data.html#read_catapult",
    "href": "blog/clean-catapult-data.html#read_catapult",
    "title": "Efficiently Cleaning Catapult Data",
    "section": "read_catapult()",
    "text": "read_catapult()\nThe above steps can also be achieved by writing a custom function. This function will load the necessary libraries, read the data, skip the meta-data, and clean the column names with 1 or 2 lines of code.\n\nFunction Breakdown\nThe function takes on two arguments: file.path and skip.rows. file.path is where the file is located, and skip.rows refers to the number of rows to skip while reading in the data. skip.rows is set to 8 by default but can be over-written as needed. Other arguments can be added to the function that mirror other arguments from the read_csv() and clean_names() functions. It is, for this example, assumed that the user wants to return the same outputs as those above.\nI’ve opted to require the readr and magrittr packagse which are a part of the tidyverse. This is lighter-weight and doesn’t require the entire tidyverse to be loaded when the function is initially run.\nThe rest of the steps are outlined within the function below using comments (starting with #).\n\nread_catapult <- function(file.path, skip.rows = 8) {\n  \n  # step 1: load libraries\n  suppressMessages(require(readr))\n  suppressMessages(require(janitor))\n  suppressMessages(require(magrittr))\n  \n  # step 2: read-in the data\n  temp <- readr::read_csv(file.path, skip = skip.rows, col_types = cols()) %>% \n    clean_names()\n  \n  # step 3: output cleaned data set\n  return(temp)\n  \n}\n\n\n\nComparing Methods\nWe should expect the read_catapult() function to return the same output as the method we used above. Although we can use the identical() function to compare the methods, it is very picky with how identical the outputs must be. What we’ll do instead is a “hack” that will return whether values are identical between methods. Only the first 6 rows will be returned using the head() function.\nThe file will be loaded from the GitHub repository (url).\n\nurl <- \"https://github.com/aaronzpearson/midsprint-blog-data/blob/main/sample_catapult.csv?raw=true\"\n\n# initial method\nreadr_fn <- read_csv(url, skip = 8, col_types = cols()) %>%\n    janitor::clean_names()\n\n# read_catapul() function\ncustom_fn <- read_catapult(file.path = url, skip.rows = 8)\n\nThe Moment of Truth\n\nhead(readr_fn == custom_fn)\n\n     timestamp seconds velocity acceleration odometer latitude longitude\n[1,]      TRUE    TRUE     TRUE         TRUE     TRUE     TRUE      TRUE\n[2,]      TRUE    TRUE     TRUE         TRUE     TRUE     TRUE      TRUE\n[3,]      TRUE    TRUE     TRUE         TRUE     TRUE     TRUE      TRUE\n[4,]      TRUE    TRUE     TRUE         TRUE     TRUE     TRUE      TRUE\n[5,]      TRUE    TRUE     TRUE         TRUE     TRUE     TRUE      TRUE\n[6,]      TRUE    TRUE     TRUE         TRUE     TRUE     TRUE      TRUE\n     heart_rate player_load\n[1,]       TRUE        TRUE\n[2,]       TRUE        TRUE\n[3,]       TRUE        TRUE\n[4,]       TRUE        TRUE\n[5,]       TRUE        TRUE\n[6,]       TRUE        TRUE\n\n\n\n\n\n\n  \n  \n    \n  \n  \n  \n    Join the Midsprint Newsletter\n  \n  \n\nWe'll never share or sell your email address. By subscribing, you agree with Revue’s Terms of Service and Privacy Policy."
  },
  {
    "objectID": "blog/fvp_split-apply-combine.html",
    "href": "blog/fvp_split-apply-combine.html",
    "title": "Profiling Multiple Athletes with fvp",
    "section": "",
    "text": "A while ago, I was asked how to use fvp to profile multiple athletes at once.\nAt the time, I had trouble working it out. Luckily, a current project required a similar task and I figured that I would try my hand at profiling multiple athletes using the fvp package again.\nAn oversight of mine is that the fvp package isn’t setup to work well with the tidyverse (this might change with future updates). Therefore, we can’t rely on the group_by(), nest() and map() combination to apply functions to multiple players at once. Instead, we can rely on base R’s equivalent of split-apply-combine which, in this case, will rely on the split(), lapply(), and rbind() functions.\nFor this blog post, we’ll look at two scenarios: 1. creating a data.frame per athlete that can then be called upon for further analyses, and 2. reporting summarized data as a single data.frame.\nWe'll never share or sell your email address. By subscribing, you agree with Revue’s Terms of Service and Privacy Policy."
  },
  {
    "objectID": "blog/fvp_split-apply-combine.html#install-and-load-packages",
    "href": "blog/fvp_split-apply-combine.html#install-and-load-packages",
    "title": "Profiling Multiple Athletes with fvp",
    "section": "Install and Load Packages",
    "text": "Install and Load Packages\nInstall the fvp package if needed:\n\ndevtools::install_github(\"aaronzpearson/fvp\")\n\nLoad the packages\n\nlibrary(fvp) # for player profiling\nlibrary(tidyverse) # for initial data cleaning\nlibrary(data.table) # for efficient lodaing of the data"
  },
  {
    "objectID": "blog/fvp_split-apply-combine.html#load-data",
    "href": "blog/fvp_split-apply-combine.html#load-data",
    "title": "Profiling Multiple Athletes with fvp",
    "section": "Load Data",
    "text": "Load Data\nWe’ll use some of the NFL’s Big Data Bowl positional tracking data for the examples. We’ll import the data directly from GitHub by identifying the raw formatted data. Since I’ve worked with the data before, I know that there are only a handful of variables that we need or should have in the data set. The variables I selected are: x & y coordinates, speed (s), player IDs (nflId), and play IDs (playId.\nPlease note that this file is large and can take a minute or two to load.\n\nurl <- \"https://github.com/nfl-football-ops/Big-Data-Bowl/blob/master/Data/tracking_gameId_2017090700.csv?raw=true\"\nnfl <- data.table::fread(url) %>% \n  select(x, y, s, nflId, playId)\n\nhead(nfl)\n\n       x     y    s   nflId playId\n1: 41.56 16.54 3.91 2495340     44\n2: 41.95 16.62 4.28 2495340     44\n3: 42.40 16.73 4.66 2495340     44\n4: 42.85 16.82 5.04 2495340     44\n5: 43.36 16.92 5.39 2495340     44\n6: 43.87 17.02 5.60 2495340     44\n\n\nSince this data set doesn’t contain an acceleration (a) variable, we’ll need to add it in using mutate(). The data is at 10Hz, so we’ll take the difference of the players’ speed between time points and divide by 1/10. Also, I set it up so that acceleration is calculated per player, per play. This way, acceleration is not calculated as a continuous vector, rolling from player to player and play to play.\n\nnfl.clean = nfl %>% \n  group_by(nflId, playId) %>% \n  mutate(a = c(0, diff(s))/.1)\n\nhead(nfl.clean)\n\n# A tibble: 6 × 6\n# Groups:   nflId, playId [1]\n      x     y     s   nflId playId     a\n  <dbl> <dbl> <dbl>   <int>  <int> <dbl>\n1  41.6  16.5  3.91 2495340     44  0   \n2  42.0  16.6  4.28 2495340     44  3.7 \n3  42.4  16.7  4.66 2495340     44  3.8 \n4  42.8  16.8  5.04 2495340     44  3.8 \n5  43.4  16.9  5.39 2495340     44  3.50\n6  43.9  17.0  5.6  2495340     44  2.1"
  },
  {
    "objectID": "blog/fvp_split-apply-combine.html#select-player",
    "href": "blog/fvp_split-apply-combine.html#select-player",
    "title": "Profiling Multiple Athletes with fvp",
    "section": "Select Player",
    "text": "Select Player\nFor our examples, we’ll select the two players with the most playing time.\n\nplayers <- nfl.clean %>%\n  group_by(nflId) %>% \n  count(sort = TRUE) \n\ntop.players <- players$nflId[2:3] # players$nflId[1] is NA; represents the football\n\ntop.nfl <- nfl.clean %>% \n  filter(nflId %in% top.players)\n\nhead(top.nfl)\n\n# A tibble: 6 × 6\n# Groups:   nflId, playId [1]\n      x     y     s   nflId playId     a\n  <dbl> <dbl> <dbl>   <int>  <int> <dbl>\n1  42.2  32.2  4.83 2550257     44  0   \n2  42.6  31.9  5.05 2550257     44  2.20\n3  43.1  31.7  5.29 2550257     44  2.40\n4  43.7  31.6  5.55 2550257     44  2.60\n5  44.3  31.4  5.84 2550257     44  2.9 \n6  44.9  31.2  6.14 2550257     44  3"
  },
  {
    "objectID": "blog/fvp_split-apply-combine.html#split-the-data-set-per-player",
    "href": "blog/fvp_split-apply-combine.html#split-the-data-set-per-player",
    "title": "Profiling Multiple Athletes with fvp",
    "section": "Split the Data Set per Player",
    "text": "Split the Data Set per Player\nThe first step is to split() the data per player by their nflId. This is iterated over all players automatically. We’ll be left with a list of data.frames, so calling head() won’t work. Instead, we need to use the apply() family of functions to iterate a function over each element of the list.\n\nplayer.df <- split(top.nfl, top.nfl$nflId)\n\nlapply(player.df, head)\n\n$`2543699`\n# A tibble: 6 × 6\n# Groups:   nflId, playId [1]\n      x     y     s   nflId playId      a\n  <dbl> <dbl> <dbl>   <int>  <int>  <dbl>\n1  38.0  28.6  0.31 2543699    395  0    \n2  38.0  28.6  0.29 2543699    395 -0.200\n3  37.9  28.6  0.27 2543699    395 -0.200\n4  37.9  28.5  0.26 2543699    395 -0.100\n5  37.9  28.5  0.24 2543699    395 -0.200\n6  37.8  28.5  0.23 2543699    395 -0.100\n\n$`2550257`\n# A tibble: 6 × 6\n# Groups:   nflId, playId [1]\n      x     y     s   nflId playId     a\n  <dbl> <dbl> <dbl>   <int>  <int> <dbl>\n1  42.2  32.2  4.83 2550257     44  0   \n2  42.6  31.9  5.05 2550257     44  2.20\n3  43.1  31.7  5.29 2550257     44  2.40\n4  43.7  31.6  5.55 2550257     44  2.60\n5  44.3  31.4  5.84 2550257     44  2.9 \n6  44.9  31.2  6.14 2550257     44  3"
  },
  {
    "objectID": "blog/fvp_split-apply-combine.html#best-sprints-data-sets",
    "href": "blog/fvp_split-apply-combine.html#best-sprints-data-sets",
    "title": "Profiling Multiple Athletes with fvp",
    "section": "Best Sprints Data Sets",
    "text": "Best Sprints Data Sets\nFor this example, we’ll build a data set per player that returns their best on-field sprint. To do so, we’ll need the gps family of functions from fvp. Specifically, we’ll call the gps.best.sprint() function to return the player’s best observed, or actual, on-field sprint.\nThe gps.best.sprint() function takes on a few arguments: the game’s speed vector, the minimum starting speed for the sprint, and the percent of the player’s max speed that they must attain for us to consider the max effort sprint be achieved. Since player’s are often bumped at the beginning of each play, I set the minimum speed to 1 yd/s. I also set the percent of max speed to 95%. From experience, setting the percentage greater than 95% returns odd results because players don’t often reach their top speed more than 1-2 times per game.\nOne of the toughest parts of using lapply() is understanding the syntax. The first argument is the list onto which we want to apply a function. They second argument is the function we want to aply. That said, the function can either be pre-existing like in the example above. Otherwise, we can build a new function to incorporate different arguments.\nBelow, we must set the gps.best.sprint() function within another function. This way, we can consider x as the element of the list we want to apply the function. For this example, x represents the athletes’ data.frames.\n\nsprints <- lapply(player.df, \n                  function(x) gps.best.sprint(game.speed =  x$s, \n                                              min.speed = 1, \n                                              max.speed.threshold = 95)\n)\n\nlapply(sprints, head)\n\n$`2543699`\n  split.time observed.speed\n1        0.0           0.00\n2        0.1           0.87\n3        0.2           1.03\n4        0.3           1.25\n5        0.4           1.49\n6        0.5           1.75\n\n$`2550257`\n  split.time observed.speed\n1        0.0           0.00\n2        0.1           0.96\n3        0.2           1.66\n4        0.3           1.92\n5        0.4           2.19\n6        0.5           2.47\n\n\nWe can then plot the sprints by either combining the data sets and adding-in the player’s names, or by calling upon their position in the list of data.frames. Here, I went with the latter.\n\ntheme_set(theme_minimal())\n\nggplot(sprints[[1]], aes(x = split.time, y = observed.speed)) +\n  geom_point() +\n  geom_point(data = sprints[[2]], colour = \"red\") +\n  ylab(\"Observed Speed (yards/ s)\") +\n  xlab(\"Split Time (s)\")"
  },
  {
    "objectID": "blog/fvp_split-apply-combine.html#player-profile-data-set",
    "href": "blog/fvp_split-apply-combine.html#player-profile-data-set",
    "title": "Profiling Multiple Athletes with fvp",
    "section": "Player Profile Data Set",
    "text": "Player Profile Data Set\nIf we wanted to have a single data set that has all of the player’s summarized data, we can use a similar approach to the one above. The final step is to then re-combine the data so it is all in a single data frame using rbind().\nWe’ll use the same data split sets from above.\nIn this code chunk, we:\n* apply the speed-accel player profiling function (sa.player.profile) to each element of the list\n* use do.call() which also applies a function to each of the elements of the list\nWe need to use do.call() because it applies a function that isn’t typically allowed with lists. In this case, we built player profiles and used do.call() and rbind() to bind them back into a single data frame.\nIf we called rbind() directly, we’ll be returned an error.\n\nspeed.accel.profiles <- do.call(rbind,\n                           lapply(player.df, \n                          function(x) sa.player.profile(player.name = unique(x$nflId), \n                                                        game.data = sa.data(x$s, x$a)))\n)\n\nspeed.accel.profiles\n\n        player.name max.speed max.accel player.tau  r.square n.obervation\n2543699     2543699  10.85789  6.833430   1.588937 0.9546985           47\n2550257     2550257  11.39854  6.775927   1.682211 0.9519218           53\n\n\nUsing the speed-accel function on the players’ data sets, we are returned their summarized data. The speed-accel profiles look similar with player 2550257 slightly faster and player 2543699 slightly more accerlation dominant.\n\n\n\n\n  \n  \n    \n  \n  \n  \n    Join the Midsprint Newsletter\n  \n  \n\nWe'll never share or sell your email address. By subscribing, you agree with Revue’s Terms of Service and Privacy Policy."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Join the Midsprint Newsletter\n\n\n\n  \n  \n\n\n\nWe'll never share or sell your email address. By subscribing, you agree with Revue’s Terms of Service and Privacy Policy.\n\nProfiling Multiple Athletes with fvp\nModelling multiple force-velocity-power profiles simultaneously using the fvp package.\nBDB 2022: The Punt Returner Shouldn’t Return Punts\nBig Data Bowl 2022 submission that tried to display that the punt returner should immediately lateral the ball to improve punt return yardage.\nModelling Cardiac Drift\nTry to predict the increase of an athlete’s heart rate over time during maintained aerobic conditioning.\nCleaning Catapult Data\nCatapult data usually has meta-data at the top of the .csv file. This blog show you how to efficiently reading and cleaning Catapult data into R. A custom function is also provided at the bottom of the blog for easy replication.\nBDB 2021: 5 Seconds or Less\nBig Data Bowl 2021 submission that modelled athletes’ sprint abilities to predict passing outcomes. This submission earned an honourable mention.\n\n\n\n\n  \n  \n    \n  \n  \n  \n    Join the Midsprint Newsletter\n  \n  \n\nWe'll never share or sell your email address. By subscribing, you agree with Revue’s Terms of Service and Privacy Policy."
  },
  {
    "objectID": "blog/la-threshold.html#lactate-threshold",
    "href": "blog/la-threshold.html#lactate-threshold",
    "title": "Modelling the Lactate Threshold",
    "section": "Lactate Threshold",
    "text": "Lactate Threshold\nThe lactate threshold is located between the moderate/hard and extreme intensity zones. It is defined as the inflection point where lactate accumulates at greater rates than the body can utilize or remove.\nThis post does not provide an in-depth discussion of the lactate threshold. Rather, data was made available on Kaggle (here) and I looked identify where the athlete’s lactate threshold might be. This was achieved through different modelling strategies to try and identify the inflection point."
  },
  {
    "objectID": "blog/la-threshold.html#loading-libraries-and-data",
    "href": "blog/la-threshold.html#loading-libraries-and-data",
    "title": "Modelling the Lactate Threshold",
    "section": "Loading Libraries and Data",
    "text": "Loading Libraries and Data\n\nLoading the Libraries\nI tried modelling the lactate threshold using linear, step, exponential, and spline models. Most of the models were achieved using base R. Building splines requires the mda package.\nThe data can be downloaded the Kaggle using the link above. Otherwise, it can be sourced from my GitHub repository.\n\nsuppressMessages(library(tidyverse)) # for tidy code\nsuppressMessages(library(gridExtra)) # for tidy plotting\nsuppressMessages(library(mda)) # for spline models\n\n\n\n\n\n\n  \n\n\n\n    \n    \n  \n\n\n  \n    \nJoin the Midsprint Newsletter\n\n\n\n  \n  \n\n\n\nWe'll never share or sell your email address. By subscribing, you agree with Revue’s Terms of Service and Privacy Policy.\n\n\n\nLoading the Data\nHaving already looked at the raw data, I noticed that variable names were not properly provided. I mitigated this issue by explicitly stating what the variable names should be while reading in the data.\n\nurl <- \"https://github.com/aaronzpearson/midsprint-blog-data/raw/main/lactate-data.csv\"\n\nla <- read_csv(url,\n              col_names = c(\"time\", \"power\", \"vo2\", \"cadence\", \"la\", \"rf\", \"hr\", \"sat\"),\n              col_types = cols())\n\n\nThe Data Structure\n\nhead(la)\n\n# A tibble: 6 × 8\n   time power   vo2 cadence    la    rf    hr   sat\n  <dbl> <dbl> <dbl>   <dbl> <dbl> <dbl> <dbl> <dbl>\n1     1     0 313.        0 0.749  18.4  52.5    99\n2     2     0 214.        0 0.748  18.0  53      99\n3     3     0 115.        0 0.747  17.5  53.5    99\n4     4     0  15.5       0 0.746  17.0  54      99\n5     5     0 248.        0 0.745  16.7  54      99\n6     6     0 480.        0 0.744  16.5  54      99"
  },
  {
    "objectID": "blog/la-threshold.html#exploratory-analyses",
    "href": "blog/la-threshold.html#exploratory-analyses",
    "title": "Modelling the Lactate Threshold",
    "section": "Exploratory Analyses",
    "text": "Exploratory Analyses\nTo get a sense of the data set, you can use the summary() function to return the min, max, median, mean, and quartiles.\n\nsummary(la)\n\n      time            power            vo2             cadence      \n Min.   :   1.0   Min.   :  0.0   Min.   :  15.53   Min.   :  0.00  \n 1st Qu.: 810.2   1st Qu.:  0.0   1st Qu.:1433.55   1st Qu.:  0.00  \n Median :1619.5   Median :100.0   Median :2139.25   Median : 94.00  \n Mean   :1619.5   Mean   :129.6   Mean   :2140.88   Mean   : 70.75  \n 3rd Qu.:2428.8   3rd Qu.:220.0   3rd Qu.:2900.40   3rd Qu.: 97.00  \n Max.   :3238.0   Max.   :350.0   Max.   :4788.30   Max.   :112.00  \n       la              rf              hr             sat       \n Min.   :0.630   Min.   : 6.92   Min.   : 49.0   Min.   :94.00  \n 1st Qu.:1.856   1st Qu.:21.08   1st Qu.:106.3   1st Qu.:96.00  \n Median :4.307   Median :24.25   Median :120.0   Median :97.00  \n Mean   :4.593   Mean   :24.90   Mean   :121.0   Mean   :96.98  \n 3rd Qu.:6.804   3rd Qu.:27.55   3rd Qu.:145.7   3rd Qu.:98.00  \n Max.   :9.790   Max.   :75.48   Max.   :177.0   Max.   :99.00  \n\n\n\nData Visualization\n\nExercise Test\nPlotting power output versus time lets us see what type of threshold tests they ran.\n\nplot(la$time, la$power) # use baseR for simpler code \nlines(la$time, la$power) # no need for fancy graphics yet\n\n\n\n\nObviously, the researchers used different types or step tests. The first test is clearly an incremental step tests whereas, second test looks like it borders a graded, or gradual, exercise test.\n\n\nLactate Levels\nWe can also plot lactate levels over time to better understand the athlete’s response to increasing power outputs.\n\nplot(la$time, la$la)\n\n\n\n\nSince the data is continuous, it seems like lactate levels were continuously sampled."
  },
  {
    "objectID": "blog/la-threshold.html#modelling-approaches",
    "href": "blog/la-threshold.html#modelling-approaches",
    "title": "Modelling the Lactate Threshold",
    "section": "Modelling Approaches",
    "text": "Modelling Approaches\nThe models we are going to use are: linear, step, polynomial, and spline.\nFirstly, we need to manually separate the tests.\n\nla1 <- filter(la, time < 1250) # separate tests\nla2 <- filter(la, time > 2000, time < 2550)\n\n\nFitting the 1st Test\nBelow, we fit the first test using our pre-chosen model:\n\n# linear regression\nfit1 <- lm(la ~ time, la1)\n\n# step\nfit2 <- lm(la ~ cut(time, 3), la1) # 3 cuts for 2 thresholds (ideally)\n\n# polynomial \nfit3 <- lm(la ~ time + poly(time, 3), la1)\n\n# spline\nfit4 <- mda::mars(la1$time, la1$la, nk = 5) # nk = 5 results in 2 cuts\n\nWe can then visualize the model fits:\n\n\nCode\ntheme_set(theme_bw()) # set the global ggplot2 theme\n\n# linear regression\np1 <- ggplot(la1, aes(time, la)) +\n  geom_point(size = 1, alpha = .2, colour = \"grey\") +\n  geom_smooth(method = \"lm\", se = FALSE, formula = y ~ x, color = \"blue\") +\n  ggtitle(\"Linear Relationship\")\n\n# step \nstep_pred <- predict(fit2, la1)\np2 <- ggplot(cbind(la1, step_pred), aes(time, la)) +\n  geom_point(size = 1, alpha = .2, colour = \"grey\") +\n  geom_line(aes(y = step_pred), size = 1, color = \"blue\") +\n  ggtitle(\"Step Function Regression\")\n\n# polynomial\np3 <- ggplot(la1, aes(time, la)) +\n  geom_point(size = 1, alpha = .2, colour = \"grey\") +\n  stat_smooth( method = \"lm\", se = FALSE, formula = y ~ poly(x, 3, raw = TRUE), color = \"blue\") +\n  ggtitle(\"3rd Degree Polynomial Regression\")\n\n# spline\np4 <- la1 %>%\n  mutate(predicted = as.vector(fit4$fitted.values)) %>%\n  ggplot(aes(time, la)) +\n  geom_point(size = 1, alpha = .2, colour = \"grey\") +\n  geom_line(aes(y = predicted), size = 1, color = \"blue\") +\n  ggtitle(\"Spline - Two knots\")\n\n# output\ngridExtra::grid.arrange(p1, p2, p3, p4, nrow = 2)\n\n\n\n\n\n\nInitial Model Fit\nAlthough some models have better fit (ex: RMSE), we want to locate the inflection point. Visually, it looks like the spline fit is probably our best approach.\nNext, we need to find the time-point at which the second inflection point occurs. With this information, we can identify the athlete’s power output at that time-point. The power output will then line-up with the athlete’s lactate threshold.\nFinding the Spline Cuts\nThe largest value is where the second inflection point, or node, is located.\n\nspline_cuts <- fit4$cuts # find the cuts\nspline_cuts\n\n     [,1]\n[1,]    0\n[2,]  820\n[3,]  820\n[4,] 1160\n[5,] 1160\n\n\nFiltering for this time point results in the following. We see that an output of 260 W corresponds with this athlete’s lactate threshold of 4.18 mmol/L. This power output is also referred to as the athlete’s critical power.\n\nfilter(la1, time == max(spline_cuts))\n\n# A tibble: 1 × 8\n   time power   vo2 cadence    la    rf    hr   sat\n  <dbl> <dbl> <dbl>   <dbl> <dbl> <dbl> <dbl> <dbl>\n1  1160   260  1974      92  4.18  23.9   151    98\n\n\nVisualizing La Threshold and Critical Power\n\n\nCode\np4 <- p4 + \n    geom_vline(aes(xintercept = 1160), colour = \"red\")\n\np4_1 <- ggplot(la1, aes(x = time, y = power)) +\n    geom_point(size = 1, alpha = 0.2, colour = \"grey\") +\n    geom_vline(aes(xintercept = 1160), colour = \"red\")\n\ngridExtra::grid.arrange(p4, p4_1)\n\n\n\n\n\n\n\n\nFitting the 2nd Test\nWe’ll gloss over the steps above and just return the results.\nFor the second test, the athlete’s lactate threshold drops to 3.77 mmol/L and critical power to 225 W.\n\nfit5 <- mda::mars(la2$time, la2$la, nk = 5)\nspline_cuts <- fit5$cuts # find the cuts\n\nfilter(la2, time == max(spline_cuts))\n\n# A tibble: 1 × 8\n   time power   vo2 cadence    la    rf    hr   sat\n  <dbl> <dbl> <dbl>   <dbl> <dbl> <dbl> <dbl> <dbl>\n1  2270   225 2697.      96  3.77  25.4   146    95\n\n\nThe resulting plot fit is slightly different. Intuitively, this makes sense because the athlete has already undergone a lactate threshold test and probably still has accumulated lactate in their system.\n\n\nCode\np5 <- la2 %>%\n  mutate(predicted = as.vector(fit5$fitted.values)) %>%\n  ggplot(aes(time, la)) +\n  geom_point(size = 1, alpha = .2, colour = \"grey\") +\n  geom_line(aes(y = predicted), size = 1, color = \"blue\") +\n  ggtitle(\"Spline - Two knots v2\")\n\np5"
  },
  {
    "objectID": "blog/la-threshold.html#final-visualization",
    "href": "blog/la-threshold.html#final-visualization",
    "title": "Modelling the Lactate Threshold",
    "section": "Final Visualization",
    "text": "Final Visualization\nThis plots below repeat those from above. They provide a little more information as to where the athlete’s critical power resides between the two testing methods.\n\n\nCode\n# 1st vs 2nd exercise test\np5 <- p5 +\n    geom_vline(aes(xintercept = 2270), colour = \"red\")\n\np5_1 <- ggplot(la2, aes(x = time, y = power)) +\n    geom_point(size = 1, alpha = 0.2, colour = \"grey\") +\n    geom_vline(aes(xintercept = 2270), colour = \"red\")\n\ngridExtra::grid.arrange(p4, p4_1)\n\n\n\n\n\nCode\ngridExtra::grid.arrange(p5, p5_1)\n\n\n\n\n\n\n\n\n\n  \n  \n    \n  \n  \n  \n    Join the Midsprint Newsletter\n  \n  \n\nWe'll never share or sell your email address. By subscribing, you agree with Revue’s Terms of Service and Privacy Policy."
  },
  {
    "objectID": "blog/marathon-kcal-expend.html",
    "href": "blog/marathon-kcal-expend.html",
    "title": "Caloric Expenditure During a Marathon",
    "section": "",
    "text": "In this post, we’ll estimate a marathon runner’s caloric expenditure throughout a marathon. This is achieved using various calculations that are derived directly, indirectly, and arithmetically.\nI’ve discussed kcal expenditure in the past, but felt like it was time to revisit the topic to add greater contect."
  },
  {
    "objectID": "blog/marathon-kcal-expend.html#load-libraries",
    "href": "blog/marathon-kcal-expend.html#load-libraries",
    "title": "Caloric Expenditure During a Marathon",
    "section": "Load Libraries",
    "text": "Load Libraries\n\nlibrary(tidyverse) # for tidy code and plots\n\n\nData Simulation\nWe’ll be simulating data for this post. We’ll generate the athlete’s speed using the rnorm() function with mean set to 3.25 m/s and sd to 0.1 to minimize large fluctations in speed over time. We can even use the log() function to simulate the athlete’s start where they ramp up in speed over the course of a few seconds (I also added some noise using the rnorm() function). Finally, we’ll have the runner complete the race in 3.6 hours (12960 seconds).\nThe data is set so that the runner’s speed was sampled every second.\n\nset.seed(2) # for reproducibility\n\nmarathon <- tibble(time_s = 1:12960,\n                   time_m = 1:12960/60,\n                   time_h = 1:12960/60/60,\n                   speed_mps = c(0, # starting speed \n                             log(1:10) + rnorm(10, 0.125, 0.05), # increase from 0 velocity and add some noise\n                             rnorm(12960 - length(0:10), mean = 3.25, sd = 0.1))\n                   )\nmarathon$speed_hph <- marathon$speed_mps * 3.6 # convert speed to kph"
  },
  {
    "objectID": "blog/marathon-kcal-expend.html#exploratory-analyses",
    "href": "blog/marathon-kcal-expend.html#exploratory-analyses",
    "title": "Caloric Expenditure During a Marathon",
    "section": "Exploratory Analyses",
    "text": "Exploratory Analyses\nSince we simulated the data, we already know the data structure. To visualize whether the runner’s speed over time remains consistent, we can plot it using simple graphics from base R.\nWe’ll only visualize the first 3 minutes that the athlete ran.\n\nplot(speed_hph ~ time_m, data = marathon[1:180, ])"
  },
  {
    "objectID": "blog/marathon-kcal-expend.html#caloric-expenditure",
    "href": "blog/marathon-kcal-expend.html#caloric-expenditure",
    "title": "Caloric Expenditure During a Marathon",
    "section": "Caloric Expenditure",
    "text": "Caloric Expenditure\nWe’ll use a few different methods of calculating caloric expenditure. The majority of the calculations originate from research by Leger and Mercier. We’ll also derive caloric expenditure from the metabolic power formula (di Prampero) which is a little trickier.\nTo properly assess kcals, we’ll need the runner’s mass. In this case, we’ll set their mass to 70 kg (154 lbs).\n\nLeger-Mercier kCal\nLeger and Mercier’s initial work modelled VO2 max utilization (VdotO2 max). We can then convert oxygen intake and output to kcals using the vo2.cal.expend function below.\nThe researchers introduced various calculations. Each calculation is based on the athlete’s speed. Since the athlete maintains a consistent pace, we’ll use the formula that was derived based on athletes running around a track.\n\nvo2.track <- function (speed.kph) \n{\n    vo2 = 3.5 * speed.kph\n    return(vo2)\n}\n\nThis is then converted to caloric expenditure:\n\nvo2.cal.expend <- function(speed.kph){ # in kcal/km*kg\n  \n  vo2 = vo2.track(speed.kph)\n  \n  ce = vo2 * 5 * 60/1 * 1/1000 * 1/speed.kph\n  ce = ifelse(ce > 1.05, 1.05, ce)\n  \n  return(ce)\n  \n}\n\n\n\ndi Prampero kCal\nMetabolic power is assessed as met_energy * speed where met.energy = (155.4 * equiv_slope^5 - 30.4 * equiv_slope^4 - 43.3 * equiv_slope^3 + 46.3 * equiv_slope^2 + 19.5 * equiv_slope + 3.6) * 1.29 which is returned in joules/kg*m Luckily, kiloJoules are converted to kcal by dividing it by 4184.\nSince metabolic energy is assessed using the athlete’s acceleration, we’ll need to calculate acceleration before returning their final caloric expenditure value.\n\nmet.energy <- function (accel) {\n  \n    torso <- atan(9.81/accel)\n    \n    equiv_slope <- tan(pi/2 - torso)\n    \n    met_energy = (155.4 * equiv_slope^5 - 30.4 * equiv_slope^4 - \n        43.3 * equiv_slope^3 + 46.3 * equiv_slope^2 + 19.5 * \n        equiv_slope + 3.6) * 1.29\n    \n    return(met_energy)\n}\n\nmet.cal.expend <- function(met.energy) { \n  \n  ce = met.energy/4184 # j to kcal\n  \n}"
  },
  {
    "objectID": "blog/marathon-kcal-expend.html#caloric-expenditure-1",
    "href": "blog/marathon-kcal-expend.html#caloric-expenditure-1",
    "title": "Caloric Expenditure During a Marathon",
    "section": "Caloric Expenditure",
    "text": "Caloric Expenditure\n\nCalculating Acceleration\nVO2max utilization is returned in mL/kg*m. To calculate total caloric expenditure, we’ll need to add a distance variable to the data set in meters. Since time is in seconds, we’ll multiple speed by 1 to return the total distance covered in that second. distance should, in this case, match the athlete’s speed in m/s.\nMetabolic power takes on acceleration in m/s/s. To calculate acceleration, we need to take the difference in speed at times \\(i\\) and \\(i - 1\\) and divide by the difference in time. Since time is in seconds, we can calculate acceleration in m/s/s.\n\nbody.mass <- 70 # in kg\n\nmarathon$distance_m <- marathon$speed_mps * 1 \n\nmarathon$accel <- c(0, diff(marathon$speed_mps))/1\n\n\n\nkCal from VO2\nInstantaneous caloric expenditure (normalized) vector:\n\ncals.vo2 <- vo2.cal.expend(marathon$speed_hph)\nmarathon$cals.vo2 <- cals.vo2\n\nInstantaneous kCals (non-normalized) based on the athlete’s body mass and distance covered, and cumulative kCals are assessed below:\n\n# marathon <- marathon %>% \n#   mutate(cals.vo2.inst = cals.vo2 * distance_km * body.mass,\n#          total.cal.vo2 = cumsum(cals.vo2.inst))\n\n\n\nkCal from Metabolic Energy\nSince we’ve already calculted acceleration, all we need to do now is repeat the steps above using the met.energy and met.energy.expend functions.\n\n# cals.met.energy <- met.cal.expend(marathon$accel)\n# marathon$met.energy <- cals.met.energy\n# \n# marathon <- marathon %>% \n#   mutate(cals.met.energy.inst = met.energy * body.mass * distance_m,\n#          total.cal.met.energy = cumsum(cals.met.energy.inst, na.rm = TRUE))"
  },
  {
    "objectID": "blog/marathon-kcal-expend.html#total-calories-burned",
    "href": "blog/marathon-kcal-expend.html#total-calories-burned",
    "title": "Caloric Expenditure During a Marathon",
    "section": "Total Calories Burned",
    "text": "Total Calories Burned\nFinally, we can pull the athlete’s total caloric expenditure by using the max() function. By summing the athlete’s total kCal output, the final value is their total expenditure.\n\n# print(\"kCal from VO2: \", max(marathon$total.cal.vo2, na.rm = TRUE),\n#       \"\\nkCal from metabolic energy: \", max(marathon$total.cal.met.energy))\n\n\n# plot(marathon$total.cal.met.energy)"
  },
  {
    "objectID": "disclaimer.html",
    "href": "disclaimer.html",
    "title": "Disclaimer",
    "section": "",
    "text": "Last updated September, 2022"
  },
  {
    "objectID": "disclaimer.html#website-disclaimer",
    "href": "disclaimer.html#website-disclaimer",
    "title": "Disclaimer",
    "section": "WEBSITE DISCLAIMER",
    "text": "WEBSITE DISCLAIMER\nThe information provided by Midsprint Sports Science Consulting (“we,” “us,” or “our”) on https://midsprint.io (the “Site”) is for general informational purposes only. All information on the Site is provided in good faith, however we make norepresentation or warranty of any kind, express or implied, regarding the accuracy, adequacy, validity, reliability,availability, or completeness of any information on the Site. UNDER NO CIRCUMSTANCE SHALL WE HAVE ANYLIABILITY TO YOU FOR ANY LOSS OR DAMAGE OF ANY KIND INCURRED AS A RESULT OF THE USE OF THE SITE OR RELIANCE ON ANY INFORMATION PROVIDED ON THE SITE. YOUR USE OF THE SITE AND YOURRELIANCE ON ANY INFORMATION ON THE SITE IS SOLELY AT YOUR OWN RISK."
  },
  {
    "objectID": "disclaimer.html#external-links-disclaimer",
    "href": "disclaimer.html#external-links-disclaimer",
    "title": "Disclaimer",
    "section": "EXTERNAL LINKS DISCLAIMER",
    "text": "EXTERNAL LINKS DISCLAIMER\nThe Site may contain (or you may be sent through the Site) links to other websites or content belonging to or originatingfrom third parties or links to websites and features in banners or other advertising. Such external links are notinvestigated, monitored, or checked for accuracy, adequacy, validity, reliability, availability, or completeness by us. WE DONOT WARRANT, ENDORSE, GUARANTEE, OR ASSUME RESPONSIBILITY FOR THE ACCURACY OR RELIABILITYOF ANY INFORMATION OFFERED BY THIRD-PARTY WEBSITES LINKED THROUGH THE SITE OR ANY WEBSITE OR FEATURE LINKED IN ANY BANNER OR OTHER ADVERTISING. WE WILL NOT BE A PARTY TO OR IN ANY WAYBE RESPONSIBLE FOR MONITORING ANY TRANSACTION BETWEEN YOU AND THIRD-PARTY PROVIDERS OFPRODUCTS OR SERVICES."
  },
  {
    "objectID": "disclaimer.html#professional-disclaimer",
    "href": "disclaimer.html#professional-disclaimer",
    "title": "Disclaimer",
    "section": "PROFESSIONAL DISCLAIMER",
    "text": "PROFESSIONAL DISCLAIMER\nThe Site cannot and does not contain fitness advice. The fitness information is provided for general informational andeducational purposes only and is not a substitute for professional advice. Accordingly, before taking any actions basedupon such information, we encourage you to consult with the appropriate professionals. We do not provide any kind of fitness advice. THE USE OR RELIANCE OF ANY INFORMATION CONTAINED ON THE SITE IS SOLELY AT YOUROWN RISK."
  },
  {
    "objectID": "disclaimer.html#affiliates-disclaimer",
    "href": "disclaimer.html#affiliates-disclaimer",
    "title": "Disclaimer",
    "section": "AFFILIATES DISCLAIMER",
    "text": "AFFILIATES DISCLAIMER\nThe Site may contain links to affiliate websites, and we receive an affiliate commission for any purchases made by you on the affiliate website using such links. Our affiliates may include the following:\nAvantLink\nAwin (Affiliate Window)\nCJ Affiliate by Conversant\nClickbank\neBay Partner Network\nFlexOffers\niDevAffiliate\nMaxBounty\nRakuten Affiliate Network\nRevenueWire\nShareASale\nRefersion\nWe are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed toprovide a means for us to earn advertising fees by linking to Amazon.com and affiliated websites."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Midsprint",
    "section": "",
    "text": "We'll never share or sell your email address. By subscribing, you agree with Revue’s Terms of Service and Privacy Policy."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Midsprint",
    "section": "Education",
    "text": "Education\nMSc Exercise Science\nUniversity of Montreal | Montreal, QC\nBSc Kinesiology & Statistics\nSimon Fraser University | Vancouver, BC"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Midsprint",
    "section": "Experience",
    "text": "Experience\nSports Science Intern | Montreal Canadiens Hockey Club (NHL)\nSept 2020 - August 2022\nSports Science Consultant | Various collegiate and professional teams\nSept 2019 - Present"
  }
]